# src/merge_lora.py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import os

def main():
    # 1. è·¯å¾„è®¾ç½®
    # æ³¨æ„ï¼šå¦‚æœä½ çš„ base_model æ˜¯ä¸‹è½½åœ¨æœ¬åœ° models æ–‡ä»¶å¤¹çš„ï¼Œè¿™é‡Œæ›¿æ¢æˆæœ¬åœ°è·¯å¾„
    base_model_path = "Qwen/Qwen2.5-3B-Instruct" 
    lora_path = "./qwen2.5-3b-dpo-output/final_checkpoint"
    output_path = "./models/Qwen2.5-3B-DPO-Merged"

    print(f"[*] æ­£åœ¨åŠ è½½åŸºåº§æ¨¡å‹ (Base Model): {base_model_path} ...")
    # åŠ è½½åŸç‰ˆæ¨¡å‹
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        torch_dtype=torch.bfloat16,
        device_map="auto", # è®© transformers è‡ªåŠ¨åˆ†é…æ˜¾å­˜
    )
    
    print("[*] æ­£åœ¨åŠ è½½ Tokenizer ...")
    tokenizer = AutoTokenizer.from_pretrained(base_model_path)

    print(f"[*] æ­£åœ¨åŠ è½½ LoRA æƒé‡: {lora_path} ...")
    # æŠŠ LoRA æŒ‚è½½åˆ°åŸºåº§æ¨¡å‹ä¸Š
    model = PeftModel.from_pretrained(base_model, lora_path)
    
    print("[*] å¼€å§‹æ‰§è¡Œèåˆ (merge_and_unload) ... è¿™å¯èƒ½éœ€è¦ä¸€ä¸¤åˆ†é’Ÿã€‚")
    # æ ¸å¿ƒæ“ä½œï¼šå°† LoRA çŸ©é˜µä¹˜æ³•åˆå¹¶è¿›åŸæœ¬çš„ Linear å±‚
    model = model.merge_and_unload()

    print(f"[*] èåˆå®Œæˆï¼æ­£åœ¨ä¿å­˜å®Œæ•´æ¨¡å‹åˆ°: {output_path} ...")
    os.makedirs(output_path, exist_ok=True)
    model.save_pretrained(output_path, safe_serialization=True) # ä¿å­˜ä¸º safetensors æ ¼å¼
    tokenizer.save_pretrained(output_path)
    
    print("[*] æ‰€æœ‰æ­¥éª¤å®Œæˆï¼ä½ ç°åœ¨æ‹¥æœ‰äº†ä¸€ä¸ªå®Œå…¨ç‹¬ç«‹çš„ DPO æ¨¡å‹ï¼ğŸ‰")

if __name__ == "__main__":
    main()